---
title: "Introduction to excessmort"
author: "Rafael Irizarry and Rolando Acosta"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to excessmort}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3)
```

This is an introduction to excessmort package for analyzing time series count data. The packages was designed to help estimate excess mortality from daily death count data. It also works for  weekly and monthly data and outcomes other than death.

In this vignetter we will use the following packages

```{r}
library(dplyr)
library(ggplot2)
dslabs::ds_theme_set()
library(excessmort)
```

# Data types

There are two main data types that the package works with:

* records - Each row represents a death and includes individual level information. 
* count tables - Each row represents a date and includes a count and population size.

If you start with record-level data, it is useful to also have a dataframe with population sizes for groups of interest. The package functions expect a population size for each date and provides a the function `approx_demographics` for interpolation.

## Record-level data

As an example of record-level data we include the `cook-records` dataset.

```{r}
data("cook_records")
head(cook_records)
```

Note that this also loads an example of demographic data table:

```{r}
head(cook_demographics)
```

If you have record-level data, a first step in the analysis is to convert it to count level data. We provide the `compute_counts` function to help with this:

```{r}
counts <- compute_counts(cook_records)
head(counts)
```

The `demo` argument permits you to include demographic information so that the population size can be computed:

```{r}
counts <- compute_counts(cook_records, demo = cook_demographics)
head(counts)
```

Note that the table provided to the `demo` argument must have population size for each date of interest. The function `approx_demographics` can interpolate yearly data into daily data. The function `get_demographics` can help you get data directly from The US Census. A Census API key is required which you can obtain at [http://api.census.gov/data/key_signup.html](http://api.census.gov/data/key_signup.html), and then supply the key to the `census_api_key` function to use it throughout your tidycensus session.

The `compute_counts` function has a special argument to define agegroups which you can do like this:

```{r}
counts <- compute_counts(cook_records, by = "agegroup", demo = cook_demographics, 
                         breaks = c(0,20,40,60,80,Inf))
head(counts)
```

The breaks need to be a subset of the breaks used in the demographic data frame:

```{r}
unique(cook_demographics$agegroup)
```


You can also obtain counts for different groups as long as these variables are included in the records-level data. A population size will be provided as long as the demographic variables match.

```{r}
counts <- compute_counts(cook_records, by = c("agegroup", "race", "sex"), 
                         demo = cook_demographics, 
                         breaks = c(0,20,40,60,80,Inf))
head(counts)
```


## Count-level data

Count-level data are assumed to have at least three columns: `date`, `outcome` and `population`. These exact names need to be used for some of the package functions to work.

The package includes several examples of count-level data:

|Dataset | Description|
|---------|-----------------------|
|cdc_state_counts     |      Weekly death counts for each USA state|
|florida_counts       |     Florida daily mortality|
|icd (puerto_rico_icd) |    Puerto Rico daily mortality by cause of death|
|louisiana_counts     |   Louisiana daily mortality|
|new_jersey_counts    |  New Jersey daily mortality|
|puerto_rico_counts   | Puerto Rico daily mortality|
|puerto_rico_icd       | Puerto Rico daily mortality by cause of death|


# Computing Expected counts

A first step in most analyses is to estimate the expected count at each date. The `compute_expected` function does this. We do this by assuming the counts $Y_t$ are an overdispresed Poisson random variable with expected value

$$  
\mu_t =N_t \exp[\alpha(t) + s(t) + w(t)]
$$
with $N_t$ the population at time $t$, $\alpha(t)$ a slow trend to account for the increase in life expectancy we have seen in the last few decades, a seasonal trend $s(t)$ to account for more deaths during the winter, and a day of the week effect $w(t)$.

Because we are often fitting this model to estimate the effect of a natural disaster or outbreak, we exclude dates with special events when estimating these parameters. We do this through the `exclude` argument.

Here is an example fitting this model to Massachusetts weekly data from 2017 to 2020. We exclude the 2018 flu season and the 2020 COVID-19 pandemic.

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(lubridate)
exclude_dates <- c(seq(make_date(2017, 12, 16), make_date(2018, 1, 16), by = "day"),
                   seq(make_date(2020, 1, 1), max(cdc_state_counts$date), by = "day"))
```

The `compute_expected` function returns another count data table but with expected counts as a new column along with a column that tells us if the date was excluded in the estimation procedure:

```{r}
data("cdc_state_counts")
counts <- cdc_state_counts %>% filter(state == "Massachusetts") %>%
  compute_expected(exclude = exclude_dates, weekday.effect = FALSE)
head(counts)
```

You can make a quick plot showing the expected and observed data using the `expected_plot` function:

```{r}
expected_plot(counts)
```

You can clearly see the effects of the COVID-19 epidemic.


Note that the dispersion parameter is saved as an attribute:

```{r}
attr(counts, "dispersion")
```

If you want the estimated trend and seasonal effects to be returned you can use the `keep.components` argument:


```{r}
res  <- cdc_state_counts %>% filter(state == "Massachusetts") %>%
  compute_expected(exclude = exclude_dates, weekday.effect = FALSE,
                   keep.components = TRUE)
```

But the returned object is no longer a table but a list with the counts in the `counts` component. You can explore the trend like this:

```{r}
qplot(res$counts$date, res$trend, geom = "line", 
      xlab = "Date", ylab = "Death rate")
```

and the seasonal component like this:

```{r}
qplot(day, s, data = res$seasonal, geom = "line", 
      xlab = "Day of the year", ylab = "Seconal effect")
```

# Computing event effects

Once we have estimated $\mu(t)$ we can proceed to fit a model that accounts for natural disasters or outbreaks:

$$
Y_t \mid \varepsilon_t \sim 
    \mbox{Poisson}\left\{ \mu_t \right[1 + f(t) \left]  \varepsilon_t \right\} \mbox{ for } t = 1, \dots,T
$$

with $T$ the total number of observations, $\mu_t$ the expected number of deaths at time $t$ for a typical year, $100 \times f(t)$ the percent increase at time $t$  due to an unusual event, and $\varepsilon_t$ a time series of, possibly auto-correlated, random variables representing natural variability. 

The function `excess_model` fits this. We can supply the output `compute_expected`
or we can start directly from the count table and the expected counts will be computed:

```{r}
fit <- cdc_state_counts %>% filter(state == "Massachusetts") %>%
  excess_model(exclude = exclude_dates,
               start = min(.$date),
               end = max(.$date),
               knots.per.year = 12,
               weekday.effect = FALSE,
               verbose = FALSE)
```

The `start` and `end` arguments determine what dates the model is fit to.

We can quickly see the results using:

```{r}
excess_plot(fit)
```

The function returns dates in which an above normal rate was estimated:

```{r}
fit$detected_intervals
```

We can also compute cumulative deaths from this object:

```{r}
cumu <- excess_cumulative(fit, 
                          start = make_date(2020, 3, 1),
                          end = make_date(2020, 5, 2))
cumu %>%
  ggplot(aes(date)) +
  geom_ribbon(aes(ymin = observed- 2*sd, ymax = observed + 2*sd), alpha = 0.5) +
  geom_point(aes(y = observed)) 

```

We can also use this function to obtain excess deaths for specific intervals by supplying `intervals` instead of `start` and `end`:

```{r}
intervals <- list(flu = seq(make_date(2017, 12, 16), make_date(2018, 2, 10), by = "day"),
                  covid19 = seq(make_date(2020, 3, 14), max(cdc_state_counts$date), by = "day"))
cdc_state_counts %>% filter(state == "Massachusetts") %>%
  excess_model(exclude = exclude_dates,
               interval = intervals,
               weekday.effect = FALSE,
               verbose = FALSE)
```



# Daily data

With daily data we recommend using a model that accounts for correlated data. You can do this by setting the `model` argument to `"correlated"`. To fit this model we need a contiguous interval of dates with $f=0$ to estimate the correlation structure. This interval should not be too big (default limit is 5,000 data points) as it will slow down the estimation procedure.

We demonstrate this with data from Puerto Rico. These data are provided for each age group:

```{r}
data("puerto_rico_counts")
head(puerto_rico_counts)
```

We start by collapsing the dataset into bigger agegroups using the `collapse_counts_by_age` functions:

```{r}
counts <- collapse_counts_by_age(puerto_rico_counts, 
                                 breaks = c(0, 5, 20, 40, 60, 75, Inf))
```

In this example we will only use the oldest agegroup:

```{r}
counts <- filter(counts, agegroup == "75-Inf")
```

To fit the model we will exclude several dates due to hurricanes, dubious looking data, and the Chikungunya epidemic:

```{r}
hurricane_dates        <- as.Date(c("1989-09-18","1998-09-21","2017-09-20"))
hurricane_effect_ends  <- as.Date(c("1990-03-18","1999-03-21","2018-03-20"))
names(hurricane_dates) <- c("Hugo", "Georges", "Maria")
exclude_dates <- c(seq(hurricane_dates[1], hurricane_effect_ends[1], by = "day"),
                   seq(hurricane_dates[2], hurricane_effect_ends[2], by = "day"),
                   seq(hurricane_dates[3], hurricane_effect_ends[3], by = "day"),
                   seq(as.Date("2014-09-01"), as.Date("2015-03-21"), by = "day"),
                   seq(as.Date("2001-01-01"), as.Date("2001-01-15"), by = "day"),
                   seq(as.Date("2020-01-01"), lubridate::today(), by = "day"))
```


We pick the following dates to estimate the correlation function:

```{r}
control_dates <- seq(as.Date("2002-01-01"), as.Date("2013-12-31"), by = "day")
```


We are now ready to fit the model. We do this for 4 intervals of interest:

```{r}
interval_start <- c(hurricane_dates[2],
                  hurricane_dates[3],
                  Chikungunya = make_date(2014, 8, 1),
                  Covid_19 = make_date(2020, 1, 1))
before <-c(365, 365, 365, 548) ##days before event to include
after <-c(365, 365, 365, 90) # days after event to incldue
```

For this model we can include a discontinuity which we do for the hurricanes:

```{r}
disc <- c(TRUE, TRUE, FALSE, FALSE)
```

We can fit the model to these 4 intervals as follows:

```{r}
f <- lapply(seq_along(interval_start), function(i){
  excess_model(counts,
               event = interval_start[i],
               start = interval_start[i] - before[i],
               end = interval_start[i] + after[i],
               exclude = exclude_dates,
               control.dates = control_dates,
               knots.per.year = 12,
               discontinuity = disc[i],
               model = "correlated")
})
```

We can examine the different hurricane effects.

This is Georges:

```{r}
excess_plot(f[[1]], title = names(interval_start)[1])
```

This is MarÃ­a:

```{r}
excess_plot(f[[2]],  title = names(interval_start)[2])
```

Chikungunya:

```{r}
excess_plot(f[[3]],  title = names(interval_start)[3])
```

And COVID-19

```{r}
excess_plot(f[[4]],  title = names(interval_start)[4])
```

We can compare cumulative deaths like this:

```{r}
ndays <- 365 ## days after event to include
cumu <- lapply(seq_along(interval_start), function(i){
      excess_cumulative(f[[i]],
                      start = interval_start[i],
                      end = pmin(make_date(2020, 3, 31), interval_start[i] + ndays)) %>%
      mutate(event_day = interval_start[i], event = names(interval_start)[i])
})
cumu <- do.call(rbind, cumu)
cumu %>%
  mutate(day = as.numeric(date - event_day)) %>%
  ggplot(aes(color = event, fill = event)) +
  geom_ribbon(aes(day, ymin = fitted - 2*se, ymax = fitted + 2*se), alpha = 0.25) +
  geom_point(aes(day, observed), alpha = 0.25, cex = 1) +
  geom_line(aes(day, fitted)) +
  ggtitle("Cumulative Excess Mortality")
```






